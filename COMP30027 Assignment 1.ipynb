{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### ### The University of Melbourne, School of Computing and Information Systems\n",
    "# COMP30027 Machine Learning, 2023 Semester 1\n",
    "\n",
    "## Assignment 1: Music genre classification with naive Bayes\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Student ID(s):**     `PLEASE ENTER YOUR ID(S) HERE`\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This iPython notebook is a template which you will use for your Assignment 1 submission.\n",
    "\n",
    "Marking will be applied on the four functions that are defined in this notebook, and to your responses to the questions at the end of this notebook (Submitted in a separate PDF file).\n",
    "\n",
    "**NOTE: YOU SHOULD ADD YOUR RESULTS, DIAGRAMS AND IMAGES FROM YOUR OBSERVATIONS IN THIS FILE TO YOUR REPORT (the PDF file).**\n",
    "\n",
    "You may change the prototypes of these functions, and you may write other functions, according to your requirements. We would appreciate it if the required functions were prominent/easy to find.\n",
    "\n",
    "**Adding proper comments to your code is MANDATORY. **"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import itertools\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# This function should prepare the data by reading it from a file and converting it into a useful format for training and testing\n",
    "\n",
    "def preprocess(train_file_name, test_file_name):\n",
    "    # put data into dataframes, remove the first column (filename)\n",
    "    return pd.read_csv(train_file_name).iloc[:,1:], pd.read_csv(test_file_name).iloc[:,1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# This function should calculat prior probabilities and likelihoods from the training data and using\n",
    "# them to build a naive Bayes model\n",
    "\"\"\" Calculate the prior probability of all the class labels for the naive bayes\n",
    "    model. The argument of the function is the train dataframe and the output\n",
    "    is a dictionary containing the prior probabilities of each label \"\"\"\n",
    "def calculate_prior_prob(train_df : pd.DataFrame):\n",
    "    prior_prob = {}\n",
    "    label_instances = train_df[\"label\"]\n",
    "    labels = np.unique(label_instances)\n",
    "    total_instances = label_instances.count()\n",
    "    \n",
    "    # iterate over all the labels and calculate the prior probabilities\n",
    "    for label in labels:\n",
    "        label_count = (label_instances == label).sum()\n",
    "        prior_prob[label] = label_count / total_instances\n",
    "\n",
    "    return prior_prob\n",
    "\n",
    "\"\"\" Calculate the parameters of a Gaussian distribution for likelihood \n",
    "    probabilities of all the features in the dataset conditional on the class \n",
    "    label for the model. The argument of the function is the train dataframe \n",
    "    and the output is a 2D dictionary containing the means and \n",
    "    standard deviations\n",
    "    \"\"\"\n",
    "def calculate_gaussian_parameters(train_df : pd.DataFrame):\n",
    "    gaussian_parameters = {}\n",
    "    features = train_df.columns[:-1]\n",
    "    label_instances = train_df[\"label\"]\n",
    "    labels = np.unique(label_instances)\n",
    "\n",
    "    # iterate over each feature and then each label\n",
    "    for feature in features:\n",
    "        gaussian_parameters[feature] = {}\n",
    "        feature_label_instances = train_df[[feature, \"label\"]]\n",
    "\n",
    "        for label in labels:\n",
    "            label_count = (label_instances == label).sum()\n",
    "\n",
    "            # get all the feature values that have the label and compute the\n",
    "            # mean and standard deviation\n",
    "            values_in_label = feature_label_instances[(label_instances == label)]\n",
    "            likelihood_mean = values_in_label[feature].sum() / label_count\n",
    "            likelihood_sd = np.sqrt(values_in_label[feature].apply(lambda x: (x - likelihood_mean)**2).sum() / (label_count - 1))\n",
    "            gaussian_parameters[feature][label] = (likelihood_mean, likelihood_sd)\n",
    "\n",
    "    return gaussian_parameters\n",
    "\n",
    "\"\"\" Probability density function of the Gaussian distribution. Takes the mean,\n",
    "    standard deviation and x_value and returns the value of the density function \n",
    "    at the x_value. If take_log argument is true, return the log of \n",
    "    the density value \"\"\"\n",
    "def gaussian_pdf(mean, sd, x_value, take_log = False):\n",
    "    if take_log:\n",
    "        return -(x_value - mean)**2/(2*sd**2) - np.log(sd*np.sqrt(2*np.pi))\n",
    "    else:\n",
    "        return (1/sd*np.sqrt(2*np.pi))*np.exp(-(x_value - mean)**2/(2*sd**2))\n",
    "\n",
    "def train(train_df):\n",
    "    return calculate_prior_prob(train_df), calculate_gaussian_parameters(train_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Naive Bayes Model Application\n",
    "\n",
    "# Predicts class for a test instance\n",
    "def predict_instance(instance, priors, likelihood_pdf, likelihood_pdf_parameters):    \n",
    "    labels = likelihood_pdf_parameters[list(likelihood_pdf_parameters.keys())[0]].keys()\n",
    "    best, best_score = [], None\n",
    "    for label in labels:\n",
    "        # Calculate the best score for the current class label\n",
    "        score = np.log(priors[label])\n",
    "        for feature in instance[instance.notnull()].index:\n",
    "            mean = likelihood_pdf_parameters[feature][label][0]\n",
    "            sd = likelihood_pdf_parameters[feature][label][1]\n",
    "            score += likelihood_pdf(mean, sd, instance[feature], True)\n",
    "        # Keep track of the best class labels\n",
    "        if best_score == None or score > best_score:\n",
    "            best = [label]\n",
    "            best_score = score\n",
    "        elif score == best_score:\n",
    "            best.append(label)\n",
    "    return random.choice(best)\n",
    "    \n",
    "# Predicts classes for each instance in the test dataset\n",
    "def predict(test_df, priors, likelihood_pdf, likelihood_pdf_parameters):\n",
    "    test_df_no_labels = test_df.loc[:, test_df.columns != \"label\"]\n",
    "    return test_df_no_labels.apply(lambda instance: predict_instance(instance, priors, likelihood_pdf, likelihood_pdf_parameters),\n",
    "                                   axis='columns')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# This function should evaliate the prediction performance by comparing your modelâ€™s class outputs to ground\n",
    "# truth labels\n",
    "\n",
    "\"\"\"\n",
    "    The first argument should be a series which contains the true labels of a test set.\n",
    "    The second argument should be a series which contains predicted labels of the same test set.\"\"\"\n",
    "def evaluate(true_labels, predicted_labels, positive_label, negative_label):\n",
    "    labels = [positive_label, negative_label]\n",
    "    \n",
    "    # Count the number of occurrences for each combination of true and predicted labels\n",
    "    combination_counts = pd.concat([true_labels, predicted_labels], axis=\"columns\", ignore_index=True).value_counts()\n",
    "    \n",
    "    # Set the count of any combination that wasn't seen to 0\n",
    "    index = pd.MultiIndex.from_tuples(itertools.product(labels, labels), names=[\"true_labels\", \"predicted_labels\"])\n",
    "    confusion_matrix = combination_counts.reindex(index=index, fill_value=0)\n",
    "    \n",
    "    evaluation_metrics = dict()\n",
    "    \n",
    "    # At this point there is no ambiguity regarding the meaning of these variables... give them aliases\n",
    "    m, positive, negative = confusion_matrix, positive_label, negative_label\n",
    "    print(confusion_matrix)\n",
    "    evaluation_metrics[\"accuracy\"] = (m[positive, positive] + m[negative, negative]) / m.sum()\n",
    "    evaluation_metrics[\"precision\"] = m[positive, positive] / (m[positive, positive] + m[negative, positive])\n",
    "    evaluation_metrics[\"recall\"] = m[positive, positive] / (m[positive, positive] + m[positive, negative])\n",
    "    evaluation_metrics[\"f_1\"] = 2 * evaluation_metrics[\"precision\"] * evaluation_metrics[\"recall\"] / (evaluation_metrics[\"precision\"] + evaluation_metrics[\"recall\"])\n",
    "    \n",
    "    # TODO: more than 2 classes:\n",
    "    #   - total accuracy\n",
    "    #   - macro/micro/weighted averaging:\n",
    "    #        - precision\n",
    "    #        - recall\n",
    "    #        - f1 (beta? -> what does it even do?)\n",
    "    #   - per class:\n",
    "    #        - precision\n",
    "    #        - recall\n",
    "    #        - f1 (beta? -> what does it even do?)    \n",
    "    \n",
    "    return evaluation_metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 1. Pop vs. classical music classification\n",
    "\n",
    "#### NOTE: you may develope codes or functions to help respond to the question here, but your formal answer must be submitted separately as a PDF."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q1\n",
    "Compute and report the accuracy, precision, and recall of your model (treat \"classical\" as the \"positive\" class)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-192.3654544164318\n",
      "-280.3017241614117\n",
      "-242.03013393000685\n",
      "-394.5490078093732\n",
      "-208.1500833907578\n",
      "-292.3291211092095\n",
      "-197.80955628705888\n",
      "-282.9357561112509\n",
      "-182.88854200039393\n",
      "-262.5657649616992\n",
      "-198.0946305477055\n",
      "-312.53152790092173\n",
      "-216.0678745437503\n",
      "-255.8962059092365\n",
      "-238.70948546427158\n",
      "-255.9283360024091\n",
      "-201.66518450773933\n",
      "-299.9132616985935\n",
      "-199.1418820819427\n",
      "-286.5196225268466\n",
      "-194.49585850141003\n",
      "-267.8788975213748\n",
      "-211.77753991302058\n",
      "-285.5864927086969\n",
      "-189.59679410925085\n",
      "-265.1265760374091\n",
      "-195.0125586114946\n",
      "-273.64312273752535\n",
      "-189.84111362791987\n",
      "-269.1414449606659\n",
      "-232.6896420136129\n",
      "-307.09020303734155\n",
      "-241.6368305467322\n",
      "-345.2373516935813\n",
      "-206.79773579676834\n",
      "-295.92059278833693\n",
      "-232.74363103751688\n",
      "-301.3588712287922\n",
      "-229.2453764903148\n",
      "-335.3415604038943\n",
      "-614.3841824383885\n",
      "-203.75223019740778\n",
      "-304.7882723369487\n",
      "-253.41814554489213\n",
      "-259.2809327239709\n",
      "-278.72266177091797\n",
      "-349.19803544142474\n",
      "-198.3384901741978\n",
      "-1043.410686040528\n",
      "-196.49718713418943\n",
      "-724.2066578725279\n",
      "-189.92844959050765\n",
      "-580.6873822917622\n",
      "-193.205481333627\n",
      "-899.9945651003217\n",
      "-208.84843475246228\n",
      "-666.7139737832349\n",
      "-194.33650123326686\n",
      "-822.8437079533676\n",
      "-205.90379742612396\n",
      "-456.68373743132366\n",
      "-251.59740310697845\n",
      "-829.2977524464758\n",
      "-568.6381772063928\n",
      "-1918.5498461367442\n",
      "-214.92859609771082\n",
      "-699.9531748235341\n",
      "-195.10222370132055\n",
      "-3010.4366246268737\n",
      "-219.98699625683827\n",
      "-1910.2509562927369\n",
      "-194.31974448032776\n",
      "-1045.0310186777538\n",
      "-192.82966114147425\n",
      "-1477.1833257083408\n",
      "-240.83786110700296\n",
      "-380.2327383978695\n",
      "-205.00633890465642\n",
      "-536.3040021948937\n",
      "-200.12426207879068\n",
      "-3841.951242269031\n",
      "-200.16533556834287\n",
      "-2392.9863071769823\n",
      "-199.40797709987092\n",
      "-2761.4374752528925\n",
      "-204.8593930644998\n",
      "true_labels  predicted_labels\n",
      "classical    classical           20\n",
      "             pop                  0\n",
      "pop          classical            1\n",
      "             pop                 22\n",
      "dtype: int64\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'accuracy': 0.9767441860465116,\n",
       " 'precision': 0.9523809523809523,\n",
       " 'recall': 1.0,\n",
       " 'f_1': 0.975609756097561}"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df, test_df = preprocess('COMP30027_2023_asst1_data\\pop_vs_classical_train.csv','COMP30027_2023_asst1_data\\pop_vs_classical_test.csv')\n",
    "priors, likelihood_param = train(train_df)\n",
    "predicted_labels = predict(test_df, priors, gaussian_pdf, likelihood_param)\n",
    "evaluation_metrics = evaluate(test_df['label'], predicted_labels, 'classical', 'pop')\n",
    "evaluation_metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q2\n",
    "For each of the features X below, plot the probability density functions P(X|Class = pop) and P(X|Class = classical). If you had to classify pop vs. classical music using just one of these three features, which feature would you use and why? Refer to your plots to support your answer.\n",
    "- spectral centroid mean\n",
    "- harmony mean\n",
    "- tempo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "\"\"\" Plots the graphs of the pdf that are used for the likelihood\"\"\"\n",
    "def pdf_graphs_constructor(train_df, feature, gaussian_param, ax):\n",
    "    if (np.min(train_df[feature]) >= 0):\n",
    "        lower =  0\n",
    "    else :\n",
    "        lower =  np.min(train_df[feature]) + 0.25 * np.min(train_df[feature])\n",
    "    upper = np.max(train_df[feature]) + 0.25 * np.max(train_df[feature])\n",
    "    x_values = np.arange(lower, upper, (upper - lower) / 150)\n",
    "\n",
    "    # calculate the values of the pdf using the x-value for each label.\n",
    "    for label in ['pop', 'classical']:\n",
    "        mean = gaussian_param[feature][label][0]\n",
    "        sd = gaussian_param[feature][label][1]\n",
    "        y_values = np.array([gaussian_pdf(mean, sd, x, False) for x in x_values])\n",
    "        ax.plot(x_values, y_values, label=label)\n",
    "    ax.legend()\n",
    "    ax.set_xlabel(feature)\n",
    "    ax.set_ylabel('density')\n",
    "\n",
    "#fig, axes = plt.subplots(3, 1, figsize = (6, 10),layout='constrained')\n",
    "#pdf_graphs_constructor(train_df, 'spectral_centroid_mean', likelihood, axes[0])\n",
    "#pdf_graphs_constructor(train_df, 'harmony_mean', likelihood, axes[1])\n",
    "#pdf_graphs_constructor(train_df, 'tempo', likelihood, axes[2])\n",
    "#plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 2. 10-way music genre classification\n",
    "\n",
    "#### NOTE: you may develope codes or functions to help respond to the question here, but your formal answer must be submitted separately as a PDF."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q3\n",
    "Compare the performance of the full model to a 0R baseline and a one-attribute baseline. The one-attribute baseline should be the best possible naive Bayes model which uses only a prior and a single attribute. In your write-up, explain how you implemented the 0R and one-attribute baselines."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q4\n",
    "Train and test your model with a range of training set sizes by setting up your own train/test splits. With each split, use cross-fold validation so you can report the performance on the entire dataset (1000 items). You may use built-in functions to set up cross-validation splits. In your write-up, evaluate how model performance changes with training set size."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q5\n",
    "Implement a kernel density estimate (KDE) naive Bayes model and compare its performance to your Gaussian naive Bayes model. You may use built-in functions and automatic (\"rule of thumb\") bandwidth selectors to compute the KDE probabilities, but you should implement the naive Bayes logic yourself. You should give the parameters of the KDE implementation (namely, what bandwidth(s) you used and how they were chosen) in your write-up."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q6\n",
    "Modify your naive Bayes model to handle missing attributes in the test data. Recall from lecture that you can handle missing attributes at test by skipping the missing attributes and computing the posterior probability from the non-missing attributes. Randomly delete some attributes from the provided test set to test how robust your model is to missing data. In your write-up, evaluate how your model's performance changes as the amount of missing data increases."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  },
  "vscode": {
   "interpreter": {
    "hash": "c02d4b1f0fc943c36665f65df295c576347f442449cc319275e9dfa3d9136bb5"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
