{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### ### The University of Melbourne, School of Computing and Information Systems\n",
    "# COMP30027 Machine Learning, 2023 Semester 1\n",
    "\n",
    "## Assignment 1: Music genre classification with naive Bayes\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Student ID(s):** 1072476, 1172648\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 0: Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import itertools\n",
    "import random"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data Preprocessing\n",
    "\n",
    "\"\"\"Returns data in input file names as pandas dataframes.\"\"\"\n",
    "def preprocess(train_file_name, test_file_name):\n",
    "    # put data into dataframes, remove the first column (filename)\n",
    "    return pd.read_csv(train_file_name).iloc[:,1:], pd.read_csv(test_file_name).iloc[:,1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Naive Bayes Model Training\n",
    "\n",
    "\"\"\" Calculate the prior probability of all the class labels for the naive bayes\n",
    "    model. The argument of the function is the train dataframe and the output\n",
    "    is a dictionary containing the prior probabilities of each label \"\"\"\n",
    "def calculate_prior_prob(train_df : pd.DataFrame):\n",
    "    prior_prob = {}\n",
    "    label_instances = train_df[\"label\"]\n",
    "    labels = np.unique(label_instances)\n",
    "    total_instances = label_instances.count()\n",
    "    \n",
    "    # iterate over all the labels and calculate the prior probabilities\n",
    "    for label in labels:\n",
    "        label_count = (label_instances == label).sum()\n",
    "        prior_prob[label] = label_count / total_instances\n",
    "\n",
    "    return prior_prob\n",
    "\n",
    "\"\"\" Calculate the parameters of a Gaussian distribution for likelihood \n",
    "    probabilities of all the features in the dataset conditional on the class \n",
    "    label for the model. The argument of the function is the train dataframe \n",
    "    and the output is a 2D dictionary containing the means and \n",
    "    standard deviations\n",
    "    \"\"\"\n",
    "def calculate_gaussian_parameters(train_df : pd.DataFrame):\n",
    "    gaussian_parameters = {}\n",
    "    features = train_df.columns[:-1]\n",
    "    labels = np.unique(train_df[\"label\"])\n",
    "\n",
    "    # iterate over each feature and then each label\n",
    "    for feature in features:\n",
    "        gaussian_parameters[feature] = {}\n",
    "\n",
    "        for label in labels:\n",
    "            # get all the feature values that have the label and compute the mean and standard deviation\n",
    "            instances_in_label = train_df[train_df[\"label\"] == label]\n",
    "            values_in_label = instances_in_label[feature][instances_in_label[feature].notnull()]\n",
    "            count = len(values_in_label)\n",
    "            \n",
    "            likelihood_mean = values_in_label.sum() / count\n",
    "            likelihood_sd = np.sqrt(values_in_label.apply(lambda x: (x - likelihood_mean)**2).sum() / (count - 1))\n",
    "            gaussian_parameters[feature][label] = (likelihood_mean, likelihood_sd)\n",
    "\n",
    "    return gaussian_parameters\n",
    "\n",
    "\"\"\" Probability density function of the Gaussian distribution. Takes the mean,\n",
    "    standard deviation and x_value and returns the value of the density function \n",
    "    at the x_value. If take_log argument is true, return the log of \n",
    "    the density value \"\"\"\n",
    "def gaussian_pdf(mean, sd, x_value, take_log = False):\n",
    "    if take_log:\n",
    "        return -(x_value - mean)**2/(2*sd**2) - np.log(sd*np.sqrt(2*np.pi))\n",
    "    else:\n",
    "        return (1/sd*np.sqrt(2*np.pi))*np.exp(-(x_value - mean)**2/(2*sd**2))\n",
    "\n",
    "\"\"\" Trains a Naive Bayes model based on the input dataframe. Class labels must be in a column named \\\"label\\\".\n",
    "    The model is a tuple which contains a dictionary of prior probabilities (keys are class labels), and \n",
    "    a dictionary (keys are feature names) of dictionaries (keys are class labels) which contain\n",
    "    the mean and standard deviation of the gaussian likelihood estimate.\"\"\"\n",
    "def train(train_df):\n",
    "    return calculate_prior_prob(train_df), calculate_gaussian_parameters(train_df)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Naive Bayes Model Application\n",
    "\n",
    "# Given an input Naive Bayes Model and a pdf, predicts class of a test instance.\n",
    "def predict_instance(instance, priors, likelihood_pdf = None, likelihood_pdf_parameters = None, use_gaussian_likelihood = True, likelihood_kde = None):    \n",
    "    if use_gaussian_likelihood:\n",
    "        labels = likelihood_pdf_parameters[list(likelihood_pdf_parameters.keys())[0]].keys()\n",
    "    else:\n",
    "        labels = likelihood_kde[list(likelihood_kde.keys())[0]].keys()\n",
    "    best, best_score = [], None\n",
    "    for label in labels:\n",
    "        # Calculate the best score for the current class label\n",
    "        score = np.log(priors[label])\n",
    "        for feature in instance[instance.notnull()].index:\n",
    "            if use_gaussian_likelihood:\n",
    "                mean = likelihood_pdf_parameters[feature][label][0]\n",
    "                sd = likelihood_pdf_parameters[feature][label][1]\n",
    "                score += likelihood_pdf(mean, sd, instance[feature], True)\n",
    "            else:\n",
    "                score += likelihood_kde[feature][label].score(np.array(instance[feature]).reshape(-1,1))\n",
    "        # Keep track of the best class labels\n",
    "        if best_score == None or score > best_score:\n",
    "            best = [label]\n",
    "            best_score = score\n",
    "        elif score == best_score:\n",
    "            best.append(label)\n",
    "    return random.choice(best)\n",
    "    \n",
    "# Predicts classes for each instance in the test dataset\n",
    "def predict(test_df, priors, likelihood_pdf = None, likelihood_pdf_parameters = None, use_gaussian_likelihood = True, likelihood_kde = None):\n",
    "    test_df_no_labels = test_df.loc[:, test_df.columns != \"label\"]\n",
    "    return test_df_no_labels.apply(lambda instance: predict_instance(instance, priors, likelihood_pdf, likelihood_pdf_parameters, use_gaussian_likelihood, likelihood_kde),\n",
    "                                   axis='columns')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Model Evaluation\n",
    "\n",
    "\"\"\"Calculates accuracy, precision and recall associated with the pair of true and predicted lables.\n",
    "    If the positive label is not specified then multiclass metrics are used.\"\"\"\n",
    "def evaluate(true_labels, predicted_labels, positive_label=None):\n",
    "    labels = np.unique(true_labels)\n",
    "    \n",
    "    # Count the number of occurrences for each combination of true and predicted labels\n",
    "    combination_counts = pd.concat([true_labels, predicted_labels], axis=\"columns\", ignore_index=True).value_counts()\n",
    "    \n",
    "    # Set the count of any combination that wasn't seen to 0\n",
    "    index = pd.MultiIndex.from_tuples(itertools.product(labels, labels), names=[\"true_labels\", \"predicted_labels\"])\n",
    "    confusion_matrix = combination_counts.reindex(index=index, fill_value=0)\n",
    "    \n",
    "    evaluation_metrics = dict()\n",
    "\n",
    "    # Determine the positive labels to be used in multi/single class metrics \n",
    "    if positive_label != None:\n",
    "        positive_labels = [positive_label]\n",
    "    else:\n",
    "        positive_labels = list(labels)\n",
    "        \n",
    "    # Do one vs rest for each positive label\n",
    "    precisions, recalls, f_1s = [], [], []\n",
    "    for label in positive_labels:\n",
    "        negative_labels = [other_label for other_label in labels if other_label != label]\n",
    "        \n",
    "        true_positive = confusion_matrix[label, label]\n",
    "        true_negative = sum(confusion_matrix.loc[[(negative_label, negative_label) for negative_label in negative_labels]])\n",
    "        false_positive = sum(confusion_matrix.loc[[(negative_label, label) for negative_label in negative_labels]])\n",
    "        false_negative = sum(confusion_matrix.loc[[(label, negative_label) for negative_label in negative_labels]])\n",
    "        \n",
    "        recall = true_positive / (true_positive + false_negative)\n",
    "        recalls.append(recall)\n",
    "        \n",
    "        if true_positive + false_positive != 0:\n",
    "            precision = true_positive / (true_positive + false_positive)\n",
    "            precisions.append(precision)\n",
    "            if precision + recall != 0:\n",
    "                f_1 = 2 * precision * recall / (precision + recall) \n",
    "            else:\n",
    "                f_1 = 0\n",
    "            f_1s.append(f_1)\n",
    "            \n",
    "            evaluation_key = \"stats_\" + label\n",
    "            evaluation_metrics[evaluation_key] = dict()\n",
    "            evaluation_metrics[evaluation_key][\"recall_\" + label] = recall\n",
    "            evaluation_metrics[evaluation_key][\"precision_\" + label] = precision\n",
    "            evaluation_metrics[evaluation_key][\"f_1_\" + label] = f_1\n",
    "            \n",
    "    evaluation_metrics[\"accuracy\"] = sum(confusion_matrix[label, label] for label in labels) / confusion_matrix.sum()\n",
    "    evaluation_metrics[\"confusion_matrix\"] = confusion_matrix\n",
    "    \n",
    "    if positive_label != None:\n",
    "        # There is only one element in each of precisions, recalls, f_1s\n",
    "        evaluation_metrics[\"precision\"] = precisions[0]\n",
    "        evaluation_metrics[\"recall\"] = recalls[0]\n",
    "        evaluation_metrics[\"f_1\"] = f_1s[0]\n",
    "    # Multi-class metrics are different\n",
    "    else:\n",
    "        evaluation_metrics[\"macro_precision\"] = sum(precisions) / len(precisions)\n",
    "        evaluation_metrics[\"macro_recall\"] =  sum(recalls) / len(recalls)\n",
    "        evaluation_metrics[\"macro_f_1\"] = sum(f_1s) / len(f_1s)\n",
    "    \n",
    "    return evaluation_metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 1: Task 1. Pop vs. classical music classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'stats_classical': {'recall_classical': 1.0,\n",
       "  'precision_classical': 0.9523809523809523,\n",
       "  'f_1_classical': 0.975609756097561},\n",
       " 'accuracy': 0.9767441860465116,\n",
       " 'confusion_matrix': true_labels  predicted_labels\n",
       " classical    classical           20\n",
       "              pop                  0\n",
       " pop          classical            1\n",
       "              pop                 22\n",
       " dtype: int64,\n",
       " 'precision': 0.9523809523809523,\n",
       " 'recall': 1.0,\n",
       " 'f_1': 0.975609756097561}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Compute and report the accuracy, precision, and recall of the model.\n",
    "train_df, test_df = preprocess('COMP30027_2023_asst1_data\\pop_vs_classical_train.csv','COMP30027_2023_asst1_data\\pop_vs_classical_test.csv')\n",
    "priors, likelihood_param = train(train_df)\n",
    "predicted_labels = predict(test_df, priors, gaussian_pdf, likelihood_param)\n",
    "evaluation_metrics = evaluate(test_df['label'], predicted_labels, 'classical')\n",
    "evaluation_metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "__init__() got an unexpected keyword argument 'layout'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_9992\\991899389.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     24\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     25\u001b[0m \u001b[1;31m# plot the likelihood distribution for the three features\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 26\u001b[1;33m \u001b[0mfig\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maxes\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mplt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msubplots\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m3\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfigsize\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m(\u001b[0m\u001b[1;36m20\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m5\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mlayout\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'constrained'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     27\u001b[0m \u001b[0mpdf_graphs_constructor\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain_df\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'spectral_centroid_mean'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlikelihood_param\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maxes\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     28\u001b[0m \u001b[0mpdf_graphs_constructor\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain_df\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'harmony_mean'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlikelihood_param\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maxes\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python37\\lib\\site-packages\\matplotlib\\pyplot.py\u001b[0m in \u001b[0;36msubplots\u001b[1;34m(nrows, ncols, sharex, sharey, squeeze, subplot_kw, gridspec_kw, **fig_kw)\u001b[0m\n\u001b[0;32m   1215\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1216\u001b[0m     \"\"\"\n\u001b[1;32m-> 1217\u001b[1;33m     \u001b[0mfig\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mfigure\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m**\u001b[0m\u001b[0mfig_kw\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1218\u001b[0m     axs = fig.subplots(nrows=nrows, ncols=ncols, sharex=sharex, sharey=sharey,\n\u001b[0;32m   1219\u001b[0m                        \u001b[0msqueeze\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msqueeze\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msubplot_kw\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msubplot_kw\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python37\\lib\\site-packages\\matplotlib\\pyplot.py\u001b[0m in \u001b[0;36mfigure\u001b[1;34m(num, figsize, dpi, facecolor, edgecolor, frameon, FigureClass, clear, **kwargs)\u001b[0m\n\u001b[0;32m    523\u001b[0m                                         \u001b[0mframeon\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mframeon\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    524\u001b[0m                                         \u001b[0mFigureClass\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mFigureClass\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 525\u001b[1;33m                                         **kwargs)\n\u001b[0m\u001b[0;32m    526\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    527\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mfigLabel\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python37\\lib\\site-packages\\matplotlib_inline\\backend_inline.py\u001b[0m in \u001b[0;36mnew_figure_manager\u001b[1;34m(num, FigureClass, *args, **kwargs)\u001b[0m\n\u001b[0;32m     25\u001b[0m     \u001b[0mThis\u001b[0m \u001b[0mfunction\u001b[0m \u001b[1;32mis\u001b[0m \u001b[0mpart\u001b[0m \u001b[0mof\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mAPI\u001b[0m \u001b[0mexpected\u001b[0m \u001b[0mby\u001b[0m \u001b[0mMatplotlib\u001b[0m \u001b[0mbackends\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     26\u001b[0m     \"\"\"\n\u001b[1;32m---> 27\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0mnew_figure_manager_given_figure\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnum\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mFigureClass\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     28\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     29\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mTypeError\u001b[0m: __init__() got an unexpected keyword argument 'layout'"
     ]
    }
   ],
   "source": [
    "\"\"\" Plots the graphs of the pdf that are used for the likelihood\"\"\"\n",
    "def pdf_graphs_constructor(train_df, feature, gaussian_param, ax):\n",
    "\n",
    "    # determine a range of x-values for the plots\n",
    "    if (np.min(train_df[feature]) >= 0):\n",
    "        lower =  0\n",
    "    else:\n",
    "        lower =  np.min(train_df[feature]) + 0.25 * np.min(train_df[feature])\n",
    "    upper = np.max(train_df[feature]) + 0.25 * np.max(train_df[feature])\n",
    "    x_values = np.arange(lower, upper, (upper - lower) / 150)\n",
    "\n",
    "    # for each label, calculate the values of the pdf using the x-value \n",
    "    # and plot the graph\n",
    "    for label in ['pop', 'classical']:\n",
    "        mean = gaussian_param[feature][label][0]\n",
    "        sd = gaussian_param[feature][label][1]\n",
    "        y_values = np.array([gaussian_pdf(mean, sd, x, False) for x in x_values])\n",
    "        ax.plot(x_values, y_values, label=label)\n",
    "    \n",
    "    ax.legend()\n",
    "    ax.set_xlabel(feature)\n",
    "    ax.set_ylabel('density')\n",
    "    ax.set_title('Likelihood Distributions for {}'.format(feature))\n",
    "\n",
    "# plot the likelihood distribution for the three features\n",
    "fig, axes = plt.subplots(1, 3, figsize = (20, 5),layout='constrained')\n",
    "pdf_graphs_constructor(train_df, 'spectral_centroid_mean', likelihood_param, axes[0])\n",
    "pdf_graphs_constructor(train_df, 'harmony_mean', likelihood_param, axes[1])\n",
    "pdf_graphs_constructor(train_df, 'tempo', likelihood_param, axes[2])\n",
    "plt.show()\n",
    "#fig.savefig('likelihood_plots.png')\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 2: Task 2. 10-way music genre classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'stats_blues': {'recall_blues': 0.15789473684210525,\n",
       "  'precision_blues': 0.42857142857142855,\n",
       "  'f_1_blues': 0.23076923076923078},\n",
       " 'stats_classical': {'recall_classical': 0.85,\n",
       "  'precision_classical': 0.8947368421052632,\n",
       "  'f_1_classical': 0.8717948717948718},\n",
       " 'stats_country': {'recall_country': 0.6875,\n",
       "  'precision_country': 0.3793103448275862,\n",
       "  'f_1_country': 0.4888888888888889},\n",
       " 'stats_disco': {'recall_disco': 0.45454545454545453,\n",
       "  'precision_disco': 0.47619047619047616,\n",
       "  'f_1_disco': 0.46511627906976744},\n",
       " 'stats_hiphop': {'recall_hiphop': 0.2857142857142857,\n",
       "  'precision_hiphop': 0.5,\n",
       "  'f_1_hiphop': 0.36363636363636365},\n",
       " 'stats_jazz': {'recall_jazz': 0.3333333333333333,\n",
       "  'precision_jazz': 0.5,\n",
       "  'f_1_jazz': 0.4},\n",
       " 'stats_metal': {'recall_metal': 0.9,\n",
       "  'precision_metal': 0.3829787234042553,\n",
       "  'f_1_metal': 0.5373134328358209},\n",
       " 'stats_pop': {'recall_pop': 0.6956521739130435,\n",
       "  'precision_pop': 0.8,\n",
       "  'f_1_pop': 0.7441860465116279},\n",
       " 'stats_reggae': {'recall_reggae': 0.6428571428571429,\n",
       "  'precision_reggae': 0.5625,\n",
       "  'f_1_reggae': 0.6000000000000001},\n",
       " 'stats_rock': {'recall_rock': 0.1111111111111111,\n",
       "  'precision_rock': 0.17647058823529413,\n",
       "  'f_1_rock': 0.13636363636363638},\n",
       " 'accuracy': 0.495,\n",
       " 'confusion_matrix': true_labels  predicted_labels\n",
       " blues        blues                3\n",
       "              classical            0\n",
       "              country              5\n",
       "              disco                0\n",
       "              hiphop               0\n",
       "                                  ..\n",
       " rock         jazz                 1\n",
       "              metal               10\n",
       "              pop                  0\n",
       "              reggae               0\n",
       "              rock                 3\n",
       " Length: 100, dtype: int64,\n",
       " 'macro_precision': 0.5100758403334303,\n",
       " 'macro_recall': 0.5118608238316475,\n",
       " 'macro_f_1': 0.48380687498702085}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Train and test naive bayes model on the gztan dataset\n",
    "\n",
    "train_df, test_df = preprocess('COMP30027_2023_asst1_data\\gztan_train.csv','COMP30027_2023_asst1_data\\gztan_test.csv')\n",
    "priors, likelihood_param = train(train_df)\n",
    "predicted_labels = predict(test_df, priors, gaussian_pdf, likelihood_param)\n",
    "evaluation_metrics = evaluate(test_df['label'], predicted_labels)\n",
    "evaluation_metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q3\n",
    "Compare the performance of the full model to a 0R baseline and a one-attribute baseline. The one-attribute baseline should be the best possible naive Bayes model which uses only a prior and a single attribute. In your write-up, explain how you implemented the 0R and one-attribute baselines."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" Given a collection of labels, find the most frequent label. If there are\n",
    "    multiple most frequent labels, the function randomly chooses one\n",
    "\"\"\"\n",
    "def get_most_frequent_label(labels):\n",
    "    most_frequent_labels = []\n",
    "    most_frequent_indices = []\n",
    "    max_count = 0\n",
    "    unique_labels, counts = np.unique(labels, return_counts=True)\n",
    "\n",
    "    # find the indices with the highest count\n",
    "    for i in range(0, len(counts)):\n",
    "        if counts[i] > max_count:\n",
    "            most_frequent_indices = [i]\n",
    "            max_count = counts[i]\n",
    "        elif counts[i] == max_count:\n",
    "            most_frequent_indices.append(i)\n",
    "    \n",
    "    # using the indices, find the most frequent labels        \n",
    "    for index in most_frequent_indices:\n",
    "        most_frequent_labels.append(unique_labels[index])\n",
    "\n",
    "    return random.choice(most_frequent_labels)\n",
    "\n",
    "\"\"\" Predict the labels of the test data using a 0R baseline\n",
    "\"\"\"\n",
    "def zero_r_predict(train_df, test_df):\n",
    "    most_frequent_label = get_most_frequent_label(train_df['label'])\n",
    "    return pd.Series([most_frequent_label for i in range(0, len(test_df))])\n",
    "\n",
    "\"\"\" Predict the labels of the test data using a one-attribute baseline\n",
    "\"\"\"\n",
    "def one_attribute_predict_and_evaluate(train_df, test_df):\n",
    "    attributes = train_df.columns[:-1]\n",
    "    best_accuracy = -1\n",
    "    best_attribute = ''\n",
    "    evaluate_best_attribute = {}\n",
    "\n",
    "    # for each feature, predict the label of the test data\n",
    "    for attribute in attributes:\n",
    "        one_attribute_predicted_labels = predict(test_df[[attribute, 'label']], priors, gaussian_pdf, likelihood_param)\n",
    "        evaluate_one_attribute = evaluate(test_df['label'], one_attribute_predicted_labels)\n",
    "        \n",
    "        if evaluate_one_attribute['accuracy'] > best_accuracy:\n",
    "            best_attribute = attribute\n",
    "            evaluate_best_attribute = evaluate_one_attribute\n",
    "            best_accuracy = evaluate_one_attribute['accuracy']\n",
    "\n",
    "    return best_attribute, evaluate_best_attribute\n",
    "\n",
    "most_frequent_labels, accuracies, precisions, recalls, f_1s = [], [], [], [], []\n",
    "for i in range(10):\n",
    "    zero_r_predicted_labels = zero_r_predict(train_df, test_df)\n",
    "    evaluate_zero_r = evaluate(test_df['label'], zero_r_predicted_labels)\n",
    "    most_frequent_labels.append(zero_r_predicted_labels.iloc[0])\n",
    "    accuracies.append(evaluate_zero_r['accuracy'])\n",
    "    precisions.append(evaluate_zero_r['macro_precision'])\n",
    "    recalls.append(evaluate_zero_r['macro_recall'])\n",
    "    f_1s.append(evaluate_zero_r['macro_f_1'])\n",
    "    \n",
    "print(f\"==============================\\n\"\n",
    "    + f\"0-R Baseline\\n\"\n",
    "    + f\"Most Frequent Classes: {np.unique(most_frequent_labels)}\\n\"\n",
    "    + f\"Accuracy             : {sum(accuracies) / len(accuracies)}\\n\"\n",
    "    + f\"Precision            : {sum(precisions) / len(precisions)}\\n\"\n",
    "    + f\"Recall               : {sum(recalls) / len(recalls)}\\n\"\n",
    "    + f\"F_1                  : {sum(f_1s) / len(f_1s)}\")\n",
    "\n",
    "evaluate_one_attribute = one_attribute_predict_and_evaluate(train_df, test_df)\n",
    "\n",
    "print(f\"==============================\\n\"\n",
    "    + f\"One-Attribute Baseline\\n\"\n",
    "    + f\"Attribute: {evaluate_one_attribute[0]}\\n\"\n",
    "    + f\"Accuracy : {evaluate_one_attribute[1]['accuracy']}\\n\"\n",
    "    + f\"Precision: {evaluate_one_attribute[1]['macro_precision']}\\n\"\n",
    "    + f\"Recall   : {evaluate_one_attribute[1]['macro_recall']}\\n\"\n",
    "    + f\"F_1      : {evaluate_one_attribute[1]['macro_f_1']}\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q4\n",
    "Train and test your model with a range of training set sizes by setting up your own train/test splits. With each split, use cross-fold validation so you can report the performance on the entire dataset (1000 items). You may use built-in functions to set up cross-validation splits. In your write-up, evaluate how model performance changes with training set size."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q5\n",
    "Implement a kernel density estimate (KDE) naive Bayes model and compare its performance to your Gaussian naive Bayes model. You may use built-in functions and automatic (\"rule of thumb\") bandwidth selectors to compute the KDE probabilities, but you should implement the naive Bayes logic yourself. You should give the parameters of the KDE implementation (namely, what bandwidth(s) you used and how they were chosen) in your write-up."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neighbors import KernelDensity\n",
    "\n",
    "\"\"\" Determine the kde for the likelihood probabilities of all the features in \n",
    "    the dataset conditional on the class label for the model. The argument of \n",
    "    the function is the train dataframe and the output is a 2D dictionary \n",
    "    containing the kde likelihood for feature and specific label\n",
    "\"\"\"\n",
    "def calculate_kde_likelihood(train_df : pd.DataFrame, bandwidth_rule):\n",
    "    kde_likelihoods = {}\n",
    "    features = train_df.columns[:-1]\n",
    "    label_instances = train_df[\"label\"]\n",
    "    labels = np.unique(label_instances)\n",
    "    bandwidths = []\n",
    "\n",
    "    # iterate over each feature and then each label\n",
    "    for feature in features:\n",
    "        kde_likelihoods[feature] = {}\n",
    "\n",
    "        for label in labels:\n",
    "            # get all the feature values that have the label\n",
    "            instances_in_label = train_df[train_df[\"label\"] == label]\n",
    "            values_in_label = instances_in_label[feature][instances_in_label[feature].notnull()]\n",
    "            values_in_label = np.array(values_in_label).reshape(-1,1)\n",
    "\n",
    "            if bandwidth_rule == 'silverman':\n",
    "                # calculate silverman's rule of thumb for kde bandwidth\n",
    "                sd = np.std(values_in_label)\n",
    "                iqr = np.quantile(values_in_label, 0.75) - np.quantile(values_in_label, 0.25)\n",
    "                bandwidth = 0.9*np.min([sd, iqr / 1.34])*(len(values_in_label))**(-0.2)\n",
    "            else:\n",
    "                # use a constant bandwidth\n",
    "                bandwidth = 1.0\n",
    "            \n",
    "            bandwidths.append(bandwidth)\n",
    "\n",
    "            # fit the training data to the kde\n",
    "            kde_likelihoods[feature][label] = KernelDensity(bandwidth=bandwidth, kernel='gaussian').fit(values_in_label)\n",
    "\n",
    "    return kde_likelihoods, pd.Series(bandwidths)\n",
    "\n",
    "for bandwidth_rule in ['silverman', 'constant']:\n",
    "    likelihood_kde , bandwidths = calculate_kde_likelihood(train_df, bandwidth_rule)\n",
    "    predicted_labels_kde = predict(test_df, priors, use_gaussian_likelihood= False, likelihood_kde= likelihood_kde)\n",
    "    evaluate_kde = evaluate(test_df['label'], predicted_labels_kde)\n",
    "    evaluate_kde\n",
    "    print(f\"==============================\\n\"\n",
    "        + f\"Kernel        : Gaussian\\n\"\n",
    "        + f\"Accuracy      : {evaluate_kde['accuracy']}\\n\"\n",
    "        + f\"Precision     : {evaluate_kde['macro_precision']}\\n\"\n",
    "        + f\"Recall        : {evaluate_kde['macro_recall']}\\n\"\n",
    "        + f\"F_1           : {evaluate_kde['macro_f_1']}\\n\"\n",
    "        + f\"Bandwidth Rule: {bandwidth_rule}\\n\"\n",
    "        + f\"Bandwidth statistics:\\n\"\n",
    "        + bandwidths.describe().to_string())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q6\n",
    "Modify your naive Bayes model to handle missing attributes in the test data. Recall from lecture that you can handle missing attributes at test by skipping the missing attributes and computing the posterior probability from the non-missing attributes. Randomly delete some attributes from the provided test set to test how robust your model is to missing data. In your write-up, evaluate how your model's performance changes as the amount of missing data increases."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\"\"\"Returns a copy of the input dataframe with num random (non-label) entries changed to NaN.\"\"\"\n",
    "def randomly_drop_values(x, rate):\n",
    "    assert(0 <= rate <= 1)\n",
    "    assert(x.columns[-1] == \"label\")\n",
    "    \n",
    "    m, n = x.shape[0], x.shape[1] - 1\n",
    "    num_to_drop = int(rate * m * n)\n",
    "    rows, columns = [i for i in range(m)], [j for j in range(n)]\n",
    "    \n",
    "    all_indices = list(itertools.product(rows, columns))\n",
    "    random.shuffle(all_indices)\n",
    "    indices_to_drop = all_indices[:num_to_drop]\n",
    "    \n",
    "    y = x.copy()\n",
    "    \n",
    "    for i, j in indices_to_drop:\n",
    "        y.iloc[i, j] = None\n",
    "    \n",
    "    return y\n",
    "\n",
    "\n",
    "train_df, test_df = preprocess('COMP30027_2023_asst1_data\\gztan_train.csv','COMP30027_2023_asst1_data\\gztan_test.csv')\n",
    "priors, likelihood_param = train(train_df)\n",
    "\n",
    "for rate in [0, 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1.0]:\n",
    "    N = 10\n",
    "    accuracies, precisions, recalls, f_1s = [], [], [], []\n",
    "    for _ in range(N):\n",
    "        corrupted_test_df = randomly_drop_values(test_df, rate)\n",
    "        predicted_labels = predict(corrupted_test_df, priors, gaussian_pdf, likelihood_param)\n",
    "        evaluation_metrics = evaluate(test_df['label'], predicted_labels)\n",
    "    \n",
    "        accuracies.append(evaluation_metrics['accuracy'])\n",
    "        precisions.append(evaluation_metrics['macro_precision'])\n",
    "        recalls.append(evaluation_metrics['macro_recall'])\n",
    "        f_1s.append(evaluation_metrics['macro_f_1'])\n",
    "    \n",
    "    print(f\"==============================\\n\"\n",
    "        + f\"Drop rate: {rate * 100}%\\n\"\n",
    "        + f\"Accuracy : {sum(accuracies) / len(accuracies)}\\n\"\n",
    "        + f\"Precision: {sum(precisions) / len(precisions)}\\n\"\n",
    "        + f\"Recall   : {sum(recalls) / len(recalls)}\\n\"\n",
    "        + f\"F_1      : {sum(f_1s) / len(f_1s)}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  },
  "vscode": {
   "interpreter": {
    "hash": "c02d4b1f0fc943c36665f65df295c576347f442449cc319275e9dfa3d9136bb5"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
